---
title: "Tidy Tuesday Exercise 2"
output: 
  html_document:
    toc: FALSE
---

# Tidy Tuesday Data Wrangling File 4/11

This is the TidyTuesday for the week of 4/11/2023. The Github account for the data set can be found [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-11).

The data this week comes from [The Humane League's US Egg Production dataset](https://thehumaneleague.org/article/E008R01-us-egg-production-data) by [Samara Mendez](https://samaramendez.github.io/). Dataset and code is available for this project on OSF at [US Egg Production Data Set](https://osf.io/z2gxn/).

Description of data: "In this project, they synthesize an analysis-ready data set that tracks cage-free hens and the supply of cage-free eggs relative to the overall numbers of hens and table eggs in the United States. The data set is based on reports produced by the United States Department of Agriculture (USDA), which are published weekly or monthly. They supplement these data with definitions and a taxonomy of egg products drawn from USDA and industry publications. The data include flock size (both absolute and relative) and egg production of cage-free hens as well as all table-egg-laying hens in the US, collected to understand the impact of the industry's cage-free transition on hens. Data coverage ranges from December 2007 to February 2021."

### Library

```{r}
# data prep
library(dplyr)

#modeling
library(tidymodels)
library(here)
library(rsample)
library(parsnip)
library(yardstick)
library(Metrics)
library(rpart.plot) # for visualizing a decision tree
library(vip)  # for variable importance plots
library(rpart) #rpart is used to set engine for tree model
library(glmnet)
library(ranger)
library(dials) #grid_regular
library(workflows)
library(tune)
library(xgboost)
```

### Load in the data (make sure you have the tidy_uesdayR package loaded)

```{r}
tuesdata <- tidytuesdayR::tt_load('2023-04-11')
tuesdata <- tidytuesdayR::tt_load(2023, week = 15)

eggproduction <- tuesdata$`egg-production`
cagefreepercentages <- tuesdata$`cage-free-percentages`

```

### Examining variables in data sets

```{r}
str(eggproduction)
str(cagefreepercentages)
```

## Summarizing continuous and categorical variables

### Egg Production

```{r}
#Taking a look at product type 
table(eggproduction$prod_type)
summary(eggproduction$prod_type)
```

Strong majority of table eggs compared to hatching eggs

```{r}
#Taking a look at product process
table(eggproduction$prod_process)
summary(eggproduction$prod_process)
```

There is a majority of "all" which indicates cage-free and conventional housing. While the numbers for cage-free(non-organic) and cage-free (organic) are the same.

```{r}
#Number of hens 
summary(eggproduction$n_hens)
```

```{r}
#Number of eggs
summary(eggproduction$n_eggs)
```

```{r}
#Source for egg production
table(eggproduction$source)
summary(eggproduction$source)
```

### Cage free

```{r}
#percent hens:observed or computed percentage of cage-free hens relative to all table-egg-laying hens
summary(cagefreepercentages$percent_hens)
```

When looking directly at the data there's an obvious increase of cage-free hens relative to all table-egg-laying hens.

```{r}
summary(cagefreepercentages$observed_month)
```

The increase of cage free hens raltive to table all table-egg-laying hens increases from 2007 until 2021.

```{r}
#percent eggs: computed percentage of cage-free eggs relative to all table eggs,This variable is not available for data sourced from the Egg Markets Overview report
summary(cagefreepercentages$percent_eggs)
```

There is an increase in cage-free eggs relative to all table eggs which could be due to the increase of cage-free hens relative to all table-egg-laying hens. There are some NA values here that I will have to deal with.

```{r}
#Source for cage free
table(cagefreepercentages$source)
summary(cagefreepercentages$source)
```

### Dealing with the missing for cagefreepercentages\$percent_eggs

There are 42/96 (43.75%) missing values in the variable percent_eggs within the cagefreepercentages data set. As a result, I am going to get rid of this variable.

```{r}
cagefreepercentages2 <- cagefreepercentages[, -which(names(cagefreepercentages) == "percent_eggs")]
```

### Determing a research question and an outcome and predictors as well

Research Question: There is a noticeable decrease in total egg production from 2016 to 2021. Does the decrease in total egg production have something to do with the cage-free prodcution method?

Outcome: n_eggs

Predictors: n_hens, prod_type and prod_process

# Fitting a Linear Regression Model and Using 4 ML Methods to Evaluate the Performance

### Setting a seed

```{r}
set.seed(698)
```

### Data splitting

```{r}
data_split = initial_split(eggproduction, prop = 7/10, strata = n_eggs)
```

### Splitting the data between training and testing

```{r}
train_data = training(data_split)
test_data  = testing(data_split)
```

### Doing 5-fold cross validation with 5 repeats and body temperature as the strata

```{r}
CV_fold = vfold_cv(train_data, v = 5, repeats = 5, strata = n_eggs) #
```

### Creating a recipe and applying to the training data

```{r}
recipe <- recipe(n_eggs ~ n_hens + prod_type + prod_process, data = train_data) %>%
  step_dummy(all_nominal(), one_hot = TRUE) #In this code, we create a recipe using step_dummy to preprocess the data and convert the categorical predictors into numeric predictors
```

## Creating a mean null model and evaluating the model for both the training data and test data

```{r}
mean_null_model <- function(eggproduction) {
  outcome <- mean(eggproduction$n_eggs)
  prediction <- rep(outcome, nrow(eggproduction))
  return(prediction)}

#using model to make predictions based on the training data and test data
train_pred <- mean_null_model(train_data)
test_pred <- mean_null_model(test_data)

#calculating rmse for the model fit to training data 
train_rmse <- rmse(train_data$n_eggs, train_pred)
test_rmse <- rmse(test_data$n_eggs, test_pred)

#taking a look at the rmse for both training and test data
train_rmse
test_rmse
```

## Fitting a Tree Model

### Tuning Hyperparameters

```{r}
#tuning: the process of estimating the best values for these values by training many models on resampled data sets and exploring how well all these models perform
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
```

### Creating a grid of values

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

### Setting up a tree workflow

```{r}
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(recipe)

#getting turning results 
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = CV_fold,
    grid = tree_grid
    )

tree_res %>%  collect_metrics()
```

### Plotting results of the tree model

```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Taking a look at the best tree models based on the R\^2 values

```{r}
tree_res %>%
  show_best("rsq")

best_tree = tree_res %>%
  select_best("rsq")

best_tree
```

### Finalizing model

```{r}
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
```

### Evaluate the final fit for the tree model

```{r}
final_fit <- 
  final_wf %>%
  last_fit(data_split) 

final_fit_metrics <- final_fit %>%
  collect_metrics()
#final fit metrics
final_fit_metrics

#creating a tibble for predicted values of model
predictions = final_fit  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot1 = final_fit %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = n_eggs)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals = predictions$n_eggs - predictions$.pred

#creating a residual data frame 
residuals_df = data.frame(n_eggs = predictions$n_eggs, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")

```

## Fitting for LASSO model

### Building a model

```{r}
lm_mod =
  linear_reg(penalty = tune(), mixture =1) %>%
  set_engine("glmnet")
```

### Creating a workflow

```{r}
lm_workflow =
  workflow() %>%
  add_model(lm_mod)  %>%
  add_recipe(recipe)
```

### Creating a grid for tuning

```{r}
lm_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# lowest penalty values
lm_reg_grid %>% top_n(-5)

# highest penalty values 
lm_reg_grid %>% top_n(5)

```

### Train and tune the model

```{r}
lasso_res <- lm_workflow %>%
  tune_grid(resamples = CV_fold, grid = lm_reg_grid)
```

### Collect metrics and picking the best model

```{r}
lasso_res %>% show_best("rsq")

best_lasso = lasso_res %>%
  select_best("rsq")
best_lasso
```

### Finalizing Lasso model

```{r}
final_lasso_wf <- 
  lm_workflow %>% 
  finalize_workflow(best_lasso)

final_lasso_wf
```

### Evaluate the final fit for the Lasso model

```{r}
final_fit2 <- 
  final_lasso_wf %>%
  last_fit(data_split) 

final_fit_metrics2 <- final_fit2 %>%
  collect_metrics()
final_fit_metrics2

#creating a tibble for predicted values of model
predictions2 = final_fit2  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot2 = final_fit2 %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = n_eggs)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals2 = predictions$n_eggs - predictions$.pred

#creating a residual data frame 
residuals_df2 = data.frame(n_eggs = predictions$n_eggs, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df2, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted Values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")
```

## Fitting for random forest model

### Building a model

```{r}
rf_mod =
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
```

### Creating a workflow

```{r}
rf_workflow =
  workflow() %>%
  add_model(rf_mod)  %>%
  add_recipe(recipe)
```

### Train and tune the model

```{r}
rf_res <- 
  rf_workflow %>% 
  tune_grid(CV_fold)
```

### Collect metrics and picking the best model

```{r}
rf_res %>% show_best("rsq")

best_rf = rf_res %>%
  select_best("rsq")
best_rf
```

### Finalizing random forest model

```{r}
final_rf_wf <- 
  rf_workflow %>% 
  finalize_workflow(best_rf)

final_rf_wf
```

### Evaluate the final fit for the random forest model

```{r}
final_fit3 <- 
  final_rf_wf %>%
  last_fit(data_split) 

final_fit_metrics3 <- final_fit3 %>%
  collect_metrics()
final_fit_metrics3

#creating a tibble for predicted values of model
predictions3 = final_fit3  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot3 = final_fit3 %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = n_eggs)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals3 = predictions$n_eggs - predictions$.pred

#creating a residual data frame 
residuals_df3 = data.frame(n_eggs = predictions$n_eggs, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df3, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted Values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")
```

# Discussion
All the models had high RMSE values indicating the models were not overly accurate when predicting the outcome variable of the number of eggs. I do not believe any outliers were to blame for the high RMSE values as there did not appear to be any obvious outlier when examining the variables. The reported R^2 values of the models were high, as a result this metric was used to determine the best fit for this exercise. I do understand that a high R^2 value does not necessarily indicate the best model. In this exercise the high R^2 values indicated that the models were able to explain a large percentage of variation in the outcome variable, but the high RMSE values indicated that the models were not too accurate when predicting values for the outcome.  

The following are the metrics I intend to compare for each model: 

1. The null model had an RMSE value of 3.1e+09 for the train data and 3.0e+09 for the test data  

2. The best tree model produced an RMSE value of 1.5e+08 and an R^2 value of 9.98e-01. 
3. The LASSO Model produced an RMSE value of 1.17e+08 and an R^2 value of 9.99e-01.

4. The random forest model produced an RMSE value of 1.46e+08 and an R^2 value of 9.98e-01. 

Given these RMSE and R^2 values the best overall model is the LASSO Model. The LASSO model reported the highest R^2 value indicating the model accounts for very high percentage of variability of the outcome variable, number of eggs. The RMSE of 1.17e+08 is high but is the lowest out of the three other models. It remains unclear to me why the RMSE values are that high. From my understanding, the accuracy of each model could be better. Another note to take into account is that the value of the outcome variable were large numbers. When looking back at the minimum and maximum of the n_eggs variable they were found to be 2.98e+08 and 8.6e+09. Could the fact that the models are working with an outcome variable with such large numbers be a reason the RMSE values are so high. In addition, possibly the predictors I chose to model could be an issue. These are reasons that I will reconsider and attempt to explore exactly what the reason behind the large RMSE values are.  