---
title: "Model Evaluation"
editor: visual
output:
  html_document:
    toc: FALSE
---


# Machine Learning Exercise: Comparing Null Model to a Tree, a LASSO and a Random forest model

### Library

```{r}
library(tidymodels)
library(here)
library(rsample)
library(parsnip) #building a model specification
library(yardstick)
library(dplyr)
library(Metrics)
# Helper packages
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
library(rpart)
library(glmnet)
library(ranger)
library(dials) #grid_regular
```

### Loading the data

```{r}
location = here("Fluanalysis_exercise", "data", "cleandata", "cleandata.RDS")
clean_data = readRDS(location)
```

### Setting a seed

```{r}
set.seed(123)
```

### Data splitting

```{r}
data_split = initial_split(clean_data, prop = 7/10, strata = BodyTemp)
```

### Splitting the data between training and testing

```{r}
train_data = training(data_split)
test_data  = testing(data_split)
```

### Doing 5-fold cross validation with 5 repeats and body temperature as the strata

```{r}
CV_fold = vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)
```

### Creating a recipe and applying to the training data

```{r}
recipe <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal(), one_hot = TRUE)
```

## Creating a mean null model and evaluating the model for both the training data and test data

```{r}
mean_null_model <- function(clean_data) {
  outcome <- mean(clean_data$BodyTemp)
  prediction <- rep(outcome, nrow(clean_data))
  return(prediction)}

#using model to make predictions based on the training data and test data
train_pred <- mean_null_model(train_data)
test_pred <- mean_null_model(test_data)

#calculating rmse for the model fit to training data 
train_rmse <- rmse(train_data$BodyTemp, train_pred)
test_rmse <- rmse(test_data$BodyTemp, test_pred)

#taking a look at the rmse for both training and test data
train_rmse
test_rmse
```

#### Now moving forward, the models that we build should have a lower RMSE than the RMSE generated by the null model

## Fitting a Tree Model

### Tuning Hyperparameters

```{r}
#tuning: the process of estimating the best values for these values by training many models on resampled data sets and exploring how well all these models perform
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
```

### Creating a grid of values

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

### Setting up a tree workflow

```{r}
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(recipe)

#getting turning results 
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = CV_fold,
    grid = tree_grid
    )

tree_res %>%  collect_metrics()
```

### Plotting results of the tree model

```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

#### Taking a look at the visualization of the tree models based on the RMSE metric, the best models appear to be the yellow and salmon color

### Taking a look at the best tree models

```{r}
tree_res %>%
  show_best("rmse")

best_tree = tree_res %>%
  select_best("rmse")

best_tree
```

### Finalizing model

```{r}
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
```

### Evaluate the final fit for the tree model

```{r}
final_fit <- 
  final_wf %>%
  last_fit(data_split) 

final_fit_metrics <- final_fit %>%
  collect_metrics()
#final fit metrics
final_fit_metrics

#creating a tibble for predicted values of model
predictions = final_fit  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot1 = final_fit %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = BodyTemp)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals = predictions$BodyTemp - predictions$.pred

#creating a residual data frame 
residuals_df = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")

```

RMSE Metric for the Tree model = 1.187

## Fitting for LASSO model

### Building a model 
```{r}
lm_mod =
  linear_reg(penalty = tune(), mixture =1) %>% # Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.
  set_engine("glmnet")
```

### Creating a workflow 
```{r}
lm_workflow =
  workflow() %>%
  add_model(lm_mod)  %>%
  add_recipe(recipe)
```

### Creating a grid for tuning 
```{r}
lm_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# lowest penalty values
lm_reg_grid %>% top_n(-5)

# highest penalty values 
lm_reg_grid %>% top_n(5)

```

### Train and tune the model 
```{r}
lasso_res <- lm_workflow %>%
  tune_grid(resamples = CV_fold, grid = lm_reg_grid)
```

### Collect metrics and picking the best model 
```{r}
lasso_res %>% show_best("rmse")

best_lasso = lasso_res %>%
  select_best("rmse")
best_lasso
```

### Finalizing Lasso model

```{r}
final_lasso_wf <- 
  lm_workflow %>% 
  finalize_workflow(best_lasso)

final_lasso_wf
```

### Evaluate the final fit for the Lasso model
```{r}
final_fit2 <- 
  final_lasso_wf %>%
  last_fit(data_split) 

final_fit_metrics2 <- final_fit2 %>%
  collect_metrics()
final_fit_metrics2

#creating a tibble for predicted values of model
predictions2 = final_fit2  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot2 = final_fit2 %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = BodyTemp)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals2 = predictions$BodyTemp - predictions$.pred

#creating a residual data frame 
residuals_df2 = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df2, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted Values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")
```
Metric for best fitting LASSO model = 1.16

## Fitting for random forest model

### Building a model 
```{r}
rf_mod =
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
```

## Creating a recipe 
```{r}
recipe2 <- recipe(BodyTemp ~ ., data = train_data)
```

### Creating a workflow 
```{r}
rf_workflow =
  workflow() %>%
  add_model(rf_mod)  %>%
  add_recipe(recipe2)
```

### Train and tune the model 
```{r}
rf_res <- 
  rf_workflow %>% 
  tune_grid(CV_fold)
```

### Collect metrics and picking the best model 
```{r}
rf_res %>% show_best("rmse")

best_rf = rf_res %>%
  select_best("rmse")
best_rf
```

### Finalizing random forest model

```{r}
final_rf_wf <- 
  rf_workflow %>% 
  finalize_workflow(best_rf)

final_rf_wf
```

### Evaluate the final fit for the random forest model
```{r}
final_fit3 <- 
  final_rf_wf %>%
  last_fit(data_split) 

final_fit_metrics3 <- final_fit3 %>%
  collect_metrics()
final_fit_metrics3

#creating a tibble for predicted values of model
predictions3 = final_fit3  %>%
  collect_predictions

#plotting model predictions vs actual outcome 
plot3 = final_fit3 %>%
  collect_predictions  %>%
    ggplot(aes(x = .pred, y = BodyTemp)) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Predicted Probability") +
  ylab("Actual Class") +
  ggtitle("Predicted Values vs Actual Outcome Values")
plot1

#plotting the residuals 
residuals3 = predictions$BodyTemp - predictions$.pred

#creating a residual data frame 
residuals_df3 = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)

# Create a scatter plot with ggplot for residuals 
ggplot(residuals_df3, aes(x = Predictions , y = Residuals)) +
  geom_point() +
  xlab("Predicted Values") +
  ylab("Residuals") +
  ggtitle("Residual Plot")
```
RMSE Metric for best fitting random forest model = 1.17

## Model Selection 
I will be evaluating the performance of each model based on the resulting RMSE metric. The RMSE Metric for the Tree model = 1.19, the RMSE Metric for best fitting LASSO model = 1.16, and the RMSE Metric for best fitting random forest model = 1.17. As a result, I will be selecting the LASSO model.

## Discussion
I calculated the final evaluation for each model and based on the RMSE metric the best fitting LASSO model had the lowest RMSE being 1.16. Something I would like to go back and redo when I have the time would be the residual plots, because mine came out a bit funky. 

