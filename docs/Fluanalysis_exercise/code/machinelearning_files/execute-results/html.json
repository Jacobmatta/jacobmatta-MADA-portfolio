{
  "hash": "5aaba748326945ca0d9b63b8dd6617d2",
  "result": {
    "markdown": "---\ntitle: \"Model Evaluation\"\neditor: visual\noutput:\n  html_document:\n    toc: FALSE\n---\n\n\n\n# Machine Learning Exercise: Comparing Null Model to a Tree, a LASSO and a Random forest model\n\n### Library\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.3     ✔ recipes      1.0.3\n✔ dials        1.1.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.0     ✔ tibble       3.1.8\n✔ ggplot2      3.4.0     ✔ tidyr        1.2.1\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        0.3.5     ✔ yardstick    1.1.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n\n```{.r .cell-code}\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nhere() starts at /Users/jacobmatta/Documents/R/jacobmatta-MADA-portfolio\n```\n:::\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(parsnip) #building a model specification\nlibrary(yardstick)\nlibrary(dplyr)\nlibrary(Metrics)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Metrics'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:yardstick':\n\n    accuracy, mae, mape, mase, precision, recall, rmse, smape\n```\n:::\n\n```{.r .cell-code}\n# Helper packages\nlibrary(rpart.plot)  # for visualizing a decision tree\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: rpart\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rpart'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dials':\n\n    prune\n```\n:::\n\n```{.r .cell-code}\nlibrary(vip)         # for variable importance plots\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-7\n```\n:::\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(dials) #grid_regular\n```\n:::\n\n\n### Loading the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocation = here(\"Fluanalysis_exercise\", \"data\", \"cleandata\", \"cleandata.RDS\")\nclean_data = readRDS(location)\n```\n:::\n\n\n### Setting a seed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n### Data splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split = initial_split(clean_data, prop = 7/10, strata = BodyTemp)\n```\n:::\n\n\n### Splitting the data between training and testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data = training(data_split)\ntest_data  = testing(data_split)\n```\n:::\n\n\n### Doing 5-fold cross validation with 5 repeats and body temperature as the strata\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCV_fold = vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)\n```\n:::\n\n\n### Creating a recipe and applying to the training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe <- recipe(BodyTemp ~ ., data = train_data) %>%\n  step_dummy(all_nominal(), one_hot = TRUE)\n```\n:::\n\n\n## Creating a mean null model and evaluating the model for both the training data and test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_null_model <- function(clean_data) {\n  outcome <- mean(clean_data$BodyTemp)\n  prediction <- rep(outcome, nrow(clean_data))\n  return(prediction)}\n\n#using model to make predictions based on the training data and test data\ntrain_pred <- mean_null_model(train_data)\ntest_pred <- mean_null_model(test_data)\n\n#calculating rmse for the model fit to training data \ntrain_rmse <- rmse(train_data$BodyTemp, train_pred)\ntest_rmse <- rmse(test_data$BodyTemp, test_pred)\n\n#taking a look at the rmse for both training and test data\ntrain_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.209327\n```\n:::\n\n```{.r .cell-code}\ntest_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.163334\n```\n:::\n:::\n\n\n#### Now moving forward, the models that we build should have a lower RMSE than the RMSE generated by the null model\n\n## Fitting a Tree Model\n\n### Tuning Hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#tuning: the process of estimating the best values for these values by training many models on resampled data sets and exploring how well all these models perform\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n```\n:::\n\n\n### Creating a grid of values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n```\n:::\n\n\n### Setting up a tree workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_recipe(recipe)\n\n#getting turning results \ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = CV_fold,\n    grid = tree_grid\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n```\n:::\n\n```{.r .cell-code}\ntree_res %>%  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             <dbl>      <int> <chr>   <chr>         <dbl> <int>    <dbl> <chr>  \n 1    0.0000000001          1 rmse    standard     1.19      25  0.0181  Prepro…\n 2    0.0000000001          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 3    0.0000000178          1 rmse    standard     1.19      25  0.0181  Prepro…\n 4    0.0000000178          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 5    0.00000316            1 rmse    standard     1.19      25  0.0181  Prepro…\n 6    0.00000316            1 rsq     standard     0.0361    25  0.00422 Prepro…\n 7    0.000562              1 rmse    standard     1.19      25  0.0181  Prepro…\n 8    0.000562              1 rsq     standard     0.0361    25  0.00422 Prepro…\n 9    0.1                   1 rmse    standard     1.21      25  0.0177  Prepro…\n10    0.1                   1 rsq     standard   NaN          0 NA       Prepro…\n# … with 40 more rows\n```\n:::\n:::\n\n\n### Plotting results of the tree model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 5 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n#### Taking a look at the visualization of the tree models based on the RMSE metric, the best models appear to be the yellow and salmon color\n\n### Taking a look at the best tree models\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  show_best(\"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n2    0.0000000178          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n3    0.00000316            1 rmse    standard    1.19    25  0.0181 Preprocesso…\n4    0.000562              1 rmse    standard    1.19    25  0.0181 Preprocesso…\n5    0.000562              4 rmse    standard    1.20    25  0.0189 Preprocesso…\n```\n:::\n\n```{.r .cell-code}\nbest_tree = tree_res %>%\n  select_best(\"rmse\")\n\nbest_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n```\n:::\n:::\n\n\n### Finalizing model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart \n```\n:::\n:::\n\n\n### Evaluate the final fit for the tree model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n\nfinal_fit_metrics <- final_fit %>%\n  collect_metrics()\n#final fit metrics\nfinal_fit_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard    1.19     Preprocessor1_Model1\n2 rsq     standard    0.000889 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n#creating a tibble for predicted values of model\npredictions = final_fit  %>%\n  collect_predictions\n\n#plotting model predictions vs actual outcome \nplot1 = final_fit %>%\n  collect_predictions  %>%\n    ggplot(aes(x = .pred, y = BodyTemp)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Actual Class\") +\n  ggtitle(\"Predicted Values vs Actual Outcome Values\")\nplot1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#plotting the residuals \nresiduals = predictions$BodyTemp - predictions$.pred\n\n#creating a residual data frame \nresiduals_df = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)\n\n# Create a scatter plot with ggplot for residuals \nggplot(residuals_df, aes(x = Predictions , y = Residuals)) +\n  geom_point() +\n  xlab(\"Predicted values\") +\n  ylab(\"Residuals\") +\n  ggtitle(\"Residual Plot\")\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\nRMSE Metric for the Tree model = 1.187\n\n## Fitting for LASSO model\n\n### Building a model \n\n::: {.cell}\n\n```{.r .cell-code}\nlm_mod =\n  linear_reg(penalty = tune(), mixture =1) %>% # Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.\n  set_engine(\"glmnet\")\n```\n:::\n\n\n### Creating a workflow \n\n::: {.cell}\n\n```{.r .cell-code}\nlm_workflow =\n  workflow() %>%\n  add_model(lm_mod)  %>%\n  add_recipe(recipe)\n```\n:::\n\n\n### Creating a grid for tuning \n\n::: {.cell}\n\n```{.r .cell-code}\nlm_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n\n# lowest penalty values\nlm_reg_grid %>% top_n(-5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by penalty\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n```\n:::\n\n```{.r .cell-code}\n# highest penalty values \nlm_reg_grid %>% top_n(5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by penalty\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n```\n:::\n:::\n\n\n### Train and tune the model \n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res <- lm_workflow %>%\n  tune_grid(resamples = CV_fold, grid = lm_reg_grid)\n```\n:::\n\n\n### Collect metrics and picking the best model \n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res %>% show_best(\"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0386 rmse    standard    1.16    25  0.0170 Preprocessor1_Model26\n2  0.0489 rmse    standard    1.16    25  0.0170 Preprocessor1_Model27\n3  0.0621 rmse    standard    1.16    25  0.0170 Preprocessor1_Model28\n4  0.0304 rmse    standard    1.16    25  0.0170 Preprocessor1_Model25\n5  0.0788 rmse    standard    1.16    25  0.0171 Preprocessor1_Model29\n```\n:::\n\n```{.r .cell-code}\nbest_lasso = lasso_res %>%\n  select_best(\"rmse\")\nbest_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0386 Preprocessor1_Model26\n```\n:::\n:::\n\n\n### Finalizing Lasso model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_lasso_wf <- \n  lm_workflow %>% \n  finalize_workflow(best_lasso)\n\nfinal_lasso_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.0385662042116347\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Evaluate the final fit for the Lasso model\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit2 <- \n  final_lasso_wf %>%\n  last_fit(data_split) \n\nfinal_fit_metrics2 <- final_fit2 %>%\n  collect_metrics()\nfinal_fit_metrics2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.17   Preprocessor1_Model1\n2 rsq     standard      0.0244 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n#creating a tibble for predicted values of model\npredictions2 = final_fit2  %>%\n  collect_predictions\n\n#plotting model predictions vs actual outcome \nplot2 = final_fit2 %>%\n  collect_predictions  %>%\n    ggplot(aes(x = .pred, y = BodyTemp)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Actual Class\") +\n  ggtitle(\"Predicted Values vs Actual Outcome Values\")\nplot1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#plotting the residuals \nresiduals2 = predictions$BodyTemp - predictions$.pred\n\n#creating a residual data frame \nresiduals_df2 = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)\n\n# Create a scatter plot with ggplot for residuals \nggplot(residuals_df2, aes(x = Predictions , y = Residuals)) +\n  geom_point() +\n  xlab(\"Predicted Values\") +\n  ylab(\"Residuals\") +\n  ggtitle(\"Residual Plot\")\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\nMetric for best fitting LASSO model = 1.16\n\n## Fitting for random forest model\n\n### Building a model \n\n::: {.cell}\n\n```{.r .cell-code}\nrf_mod =\n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")\n```\n:::\n\n\n## Creating a recipe \n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe2 <- recipe(BodyTemp ~ ., data = train_data)\n```\n:::\n\n\n### Creating a workflow \n\n::: {.cell}\n\n```{.r .cell-code}\nrf_workflow =\n  workflow() %>%\n  add_model(rf_mod)  %>%\n  add_recipe(recipe2)\n```\n:::\n\n\n### Train and tune the model \n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res <- \n  rf_workflow %>% \n  tune_grid(CV_fold)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n:::\n\n\n### Collect metrics and picking the best model \n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>% show_best(\"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4    32 rmse    standard    1.16    25  0.0167 Preprocessor1_Model09\n2     2    14 rmse    standard    1.16    25  0.0170 Preprocessor1_Model06\n3     8    32 rmse    standard    1.17    25  0.0164 Preprocessor1_Model10\n4     9    24 rmse    standard    1.17    25  0.0166 Preprocessor1_Model04\n5    12    20 rmse    standard    1.18    25  0.0164 Preprocessor1_Model08\n```\n:::\n\n```{.r .cell-code}\nbest_rf = rf_res %>%\n  select_best(\"rmse\")\nbest_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     4    32 Preprocessor1_Model09\n```\n:::\n:::\n\n\n### Finalizing random forest model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_rf_wf <- \n  rf_workflow %>% \n  finalize_workflow(best_rf)\n\nfinal_rf_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 4\n  trees = 1000\n  min_n = 32\n\nComputational engine: ranger \n```\n:::\n:::\n\n\n### Evaluate the final fit for the random forest model\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit3 <- \n  final_rf_wf %>%\n  last_fit(data_split) \n\nfinal_fit_metrics3 <- final_fit3 %>%\n  collect_metrics()\nfinal_fit_metrics3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard     1.18    Preprocessor1_Model1\n2 rsq     standard     0.00969 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n#creating a tibble for predicted values of model\npredictions3 = final_fit3  %>%\n  collect_predictions\n\n#plotting model predictions vs actual outcome \nplot3 = final_fit3 %>%\n  collect_predictions  %>%\n    ggplot(aes(x = .pred, y = BodyTemp)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Actual Class\") +\n  ggtitle(\"Predicted Values vs Actual Outcome Values\")\nplot1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#plotting the residuals \nresiduals3 = predictions$BodyTemp - predictions$.pred\n\n#creating a residual data frame \nresiduals_df3 = data.frame(BodyTemp = predictions$BodyTemp, Predictions = predictions$.pred, Residuals = residuals)\n\n# Create a scatter plot with ggplot for residuals \nggplot(residuals_df3, aes(x = Predictions , y = Residuals)) +\n  geom_point() +\n  xlab(\"Predicted Values\") +\n  ylab(\"Residuals\") +\n  ggtitle(\"Residual Plot\")\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-29-2.png){width=672}\n:::\n:::\n\nRMSE Metric for best fitting random forest model = 1.17\n\n## Model Selection \nI will be evaluating the performance of each model based on the resulting RMSE metric. The RMSE Metric for the Tree model = 1.19, the RMSE Metric for best fitting LASSO model = 1.16, and the RMSE Metric for best fitting random forest model = 1.17. As a result, I will be selecting the LASSO model.\n\n## Discussion\nI calculated the final evaluation for each model and based on the RMSE metric the best fitting LASSO model had the lowest RMSE being 1.16. Something I would like to go back and redo when I have the time would be the residual plots, because mine came out a bit funky. \n\n",
    "supporting": [
      "machinelearning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}